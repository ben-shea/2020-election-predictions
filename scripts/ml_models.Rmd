---
title: "Can we Predict the US Election?"
author: Ben Shea, Mukund Poddar, Nellie Ponarul and Saul Holding
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(e1071)
library(xgboost)
library(Metrics)
library(caret)
```


### Model Fitting

We will load our dataset here that has been cleaned and put together with all the scripts in the 'cleaning_scripts folder'. We will then standardise the data.

Note: many ML applications use the data from the training set only to standardise the test set. However, that is the general practice when real world examples come one at a time while the training has been performed on many more instance together. The assumption there is that the distribution of the data is uniform across the training and test examples. In our application, each election cycle is unique, and is processed together. Before a certain election year, all the counties' predictors would be collected and the predictions would be done as a batch. Hence, we can standardise data grouped by an election cycle.

```{r dataset, message = FALSE}
train = read_csv("../data/merged_final_2016.csv")
test = read_csv("../data/merged_final_2020.csv")
train = train[complete.cases(train),]
x_train = train[c(2:(length(train)-1))]
y_train = train[length(train)]
colnames(y_train) = 'actual'

test = test[complete.cases(test),]
x_test = test[c(2:(length(test)-1))]
y_test = test[length(test)]
colnames(y_test) = 'actual'

x_train = scale(x_train)
x_test = scale(x_test)
# col_means = apply(x_train, 2, mean)
# col_sds = apply(x_train, 2, sd)

# x_train = sweep(x_train, 2, col_means, FUN = "-")
# x_train = sweep(x_train, 2, col_sds, FUN = "/")
# x_test = sweep(x_test, 2, col_means, FUN = "-")
# x_test = sweep(x_test, 2, col_sds, FUN = "/")
```

We now start using different Machine Learning models to predict the election outcomes. To keep some standardisation, we will store the actual results, and the predictions from each of the models we use into the y_train and y_test dataframes created above.


#### Multiple Linear Regression  

We selected a subset of our covariates for this model using a backwards stepwise selection:
```{r mlr, eval = F}
# Set up data as dataframe for MLR
x_train_mlr <- x_train %>% data.frame() %>% bind_cols(y_train)
x_test_mlr <- x_test %>% data.frame() %>% bind_cols(y_test)
# Fit model with backwards selected variables
covs <- names(x_train)[-length(names(x_train))]
covs <- paste0(covs, collapse = " + ")

mlr_model <- lm(actual ~ popestimate + race_white + race_black + 
    race_hispanic + race_aac + age_0_to_19_years + age_20_to_39_years + 
    age_40_to_59_years + nonprofit + nondurable_goods + raw_gdp + 
    gdp_change + unemployment + dem_poll_median + dem_poll_sd + 
    rep_poll_mean + rep_poll_median + rep_poll_sd + consistency_dem, data = x_train_mlr)

test$mlr = predict(mlr_model, x_test_mlr)
```


#### Support Vector Machines  

This model creates a decision boundary between the classes being predicted using the datapoints on the edges of the class clusters. You can read more about SVMs on [wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine).
```{r svm, eval = F}
svm_model = svm(x=x_train, y=y_train)
test$svm = predict(svm_model, x_test)
```

#### XGBoost

This model is an optimized distributed gradient boosting library, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. [parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)
```{r xgboost, eval = F}
train_final <- matrix(as.numeric(data.matrix(x_train)), nrow = nrow(train))
test_final <- matrix(as.numeric(data.matrix(x_test)), nrow = nrow(test))
#train_party_outcome <- ifelse(train_complete$democrats_pct>train_complete$republicans_pct,1,0)
train_party_outcome <- train$dem_rep_ratio
train_outcome <- matrix(as.numeric(data.matrix(train_party_outcome)))

# GRID SEARCH -------------------------------------------------------------------------------
#https://www.kaggle.com/silverstone1903/xgboost-grid-search-r

searchGridSubCol <- expand.grid(subsample = c(0.5,0.7,1), 
                                colsample_bytree = c(0.5,0.7,1),
                                max_depth = c(3,4,5,6),
                                eta = c(0.1,0.2,0.3,0.4)
)

obj<- "reg:squarederror"

weightsData <- scales::rescale(train$popestimate, to=c(0,1))

system.time(
  rmseErrorsHyperparameters <- apply(searchGridSubCol, 1, function(parameterList){
    
    #Extract Parameters to test
    currentSubsampleRate <- parameterList[["subsample"]]
    currentColsampleRate <- parameterList[["colsample_bytree"]]
    currentDepth <- parameterList[["max_depth"]]
    currentEta <- parameterList[["eta"]]
    currentMinChild <- 1
    xgb_model <- xgboost(data =  x_train, label=train_outcome, nrounds = 200, 
                         verbose = 0,"max.depth" = currentDepth, "eta" = currentEta,                               
                         "subsample" = currentSubsampleRate, "colsample_bytree" = currentColsampleRate
                         , print_every_n = 20, "min_child_weight" = 1,
                         early_stopping_rounds = 20,objective=obj, eval_metric="rmse", weight = weightsData
                         )
    
    xvalidationScores <- as.data.frame(xgb_model$evaluation_log)
    trmse <- tail(xvalidationScores$train_rmse,1)
    output <- return(c(trmse, currentSubsampleRate, currentColsampleRate, currentDepth, currentEta, currentMinChild))}))

output <- as.data.frame(t(rmseErrorsHyperparameters))
varnames <- c("TrainRMSE", "SubSampRate", "ColSampRate", "Depth","eta","currentMinChild")
names(output) <- varnames
head(output)
best_params <- output[which.min(output$TrainRMSE),]
best_params
# -----------------------------------------------------------------------------------------------

#https://xgboost.readthedocs.io/en/latest/parameter.html
xgb_model = xgboost(data=x_train, 
                    label=train_outcome, 
                    missing = NaN,
                    nrounds=200,
                    verbose=0, 
                    eta=best_params$eta, 
                    max_depth=best_params$Depth, 
                    subsample=best_params$SubSampRate, 
                    min_child=best_params$currentMinChild,
                    colsample_bytree=best_params$ColSampRate,
                    objective=obj, 
                    eval_metric="rmse",
                    weight = weightsData
                    )

# train_preds <- predict(xgb_model, x_train, missing = NaN)
# rmse(train$dem_rep_ratio,train_preds)
# plot(train$dem_rep_ratio,train_preds)

# test_preds <- predict(xgb_model, x_test, missing = NaN)
test$xgb <- predict(xgb_model, test_final, missing = NaN)
xgboost_rmse <- rmse(test$dem_rep_ratio,test$xgb)
# plot(test$dem_rep_ratio,test_preds)
```  


### Random Forest
The model is utilizes decision trees, bagging, and randomness by randomly selected predetermined number of predictors at each node, for predictions.Here we use 10 fold cross validation repeated 3 times to determine predictions for a baseline model and different tuned random forest models. 
```{r}
# Combing scaled data with NON-SCALED RESPONSE VARIABLES
train_scaled <- x_train %>% data.frame() %>% bind_cols(y_train)
test_scaled <- x_test %>% data.frame() %>% bind_cols(y_test)

# Getting number of predictors
number_predictors <- dim(train_scaled)[2] - 1

### Base model
#Setting the default tuning parameters
control <- trainControl(method='repeatedcv', number=10, repeats=3)
mtry <- sqrt(number_predictors)
tunegrid <- expand.grid(.mtry=mtry)

# Initializing and fitting the random forest
rf_model_base <- train(actual ~ ., data = train_scaled, method = 'rf',
                      metric='RMSE', tuneGrid=tunegrid, trControl = control)

## Getting results into a dataframe
# df <- data.frame(preds_rf_base, train_scaled$actual)
# 
# # Plotting the results
# df %>% ggplot(aes(preds_rf_base, train_scaled.actual))+
#     geom_point() +
#     geom_abline(intercept=0, slope = 1, color = 'red') +
#     ggtitle("Base Validation Analysis") +
#     xlab("Predictions from Base Model") +
#     ylab("Acual Validation Ratios")
# 



```


```{r}
### Tuning Method 1
control <- trainControl(method='repeatedcv', number=10, repeats=3)
tunegrid_tune <- expand.grid(.mtry= c(sqrt(number_predictors), log2(number_predictors), number_predictors/3))
# Initializing and fitting the random forest
rf_model_tuned1 <- train(actual ~ ., data = train_scaled, method = 'rf',
                       metric='RMSE', tuneGrid=tunegrid_tune, trControl = control)

```


```{r, eval=FALSE}

### Tuning Method 2: Manual Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
tunegrid_tune <- expand.grid(.mtry= c(sqrt(number_predictors), log2(number_predictors), number_predictors/3))
modellist_new <- list()
seed <- 42

for (ntree in c(seq(40,600, 80))) {
    for (nodesize in c(10, 30, 60, 90)) {

    print(paste(ntree, nodesize))

    set.seed(seed)
    fit <- train(actual ~ ., data = train_scaled, method = 'rf',
                 metric='RMSE', tuneGrid=tunegrid_tune, trControl = control, ntree=ntree, nodesize=nodesize)
    key <- toString(ntree)
    modellist_new[[key]] <- fit
    }
}
# compare results


```



```{r}
# Reading results and model from manual search
modellist_new <- readRDS("../saved_models/model_list_final.rds")
results <- readRDS("../saved_models/results_rf_final.rds")
results <- resamples(modellist_new)

# Comparing the different models
rf_model_base
rf_model_tuned1
summary(results)
dotplot(results)
```


As we can see, rd_model_tuned1 performs the best with mtry=p/3. We shall use this model to predict on the test set.

```{r}
# Predicting using best model on test set and saving predictions and errors to their respecitive dataframes
test$rf <- predict(rf_model_tuned1, newdata = test_scaled)
rf_rmse <- rmse(test_scaled$actual,test$rf)
```



```{r, echo = F}
# Save out predictions so we don't run code multiple times
#save(mlr_model, svm_model, xgb_model, test, file = "../data/raw_predictions.rda")
load("../data/raw_predictions.rda")
```

 

### Outcomes

#### Predict Electoral College Split

```{r}
# electoral college
elect_tbl <- read_csv("../data/electoral_college.csv")
maine_neb <- read_csv("../data/electoral_votes_main_nebraska.csv")

# Actual results 2020
elect_tbl_actual_2020 <- read_csv("../data/electoral_college_actual_2020.csv")
maine_neb_actual_2020 <- read_csv("../data/electoral_votes_main_nebraska_actual_2020.csv")
#read in county/state crosswalk
base_county_state_fips <- read_csv("../data/Clean Data/base_county_state_fips_lkp.csv")

# Function to take a prediction and calculate electoral college split
get_electoral_college <- function(df, pred_input, elect_votes =  elect_tbl, maine_neb_elect = maine_neb){
  # roll up to State level 
  test_pred_df <- data.frame(fips = df$fips, 
                             dem_rep_ratio = df[pred_input],
                           pop_estimate = df$popestimate)
  
  names(test_pred_df) <- c("fips", "dem_rep_ratio", "pop_estimate")
    test_pred_df <- test_pred_df %>%
    mutate(
      dem_rep_ratio_votes = dem_rep_ratio*pop_estimate,
      state_fips = str_sub(fips, 1,2)
    ) %>%
    left_join(
      base_county_state_fips[c("stname","fips")], 
      by="fips")
  
  #most of electoral college
  elect_rollup <- test_pred_df %>% 
    left_join(
      elect_tbl, 
      by = c("stname" ="state")) %>% 
    group_by(state_fips, stname) %>% 
    summarize(total_dem_rep_ratio_votes = sum(dem_rep_ratio_votes),
              total_pop = sum(pop_estimate)) %>%
    mutate(
      state_win=ifelse(total_dem_rep_ratio_votes>total_pop,1,0)
      )
  
  final_elect_rollup <- elect_rollup %>% 
  left_join(elect_votes %>% 
              select(state, elect_votes), 
            by=c("stname" ="state")) %>%
  mutate(
    dem_electoral_votes = state_win*elect_votes,
    rep_electoral_votes = elect_votes - dem_electoral_votes
    )
  
  # Additional Maine/Nebraska Calculation
  #maine and nebraska
  maine_neb_elect$fips <- as.character(maine_neb_elect$fips)
  elect_rollup_mn <- maine_neb_elect %>% left_join(test_pred_df, by = c("fips","state"="stname"))
  elect_rollup_mn <- elect_rollup_mn[complete.cases(elect_rollup_mn), ] %>% 
    group_by(state,congress_district) %>% summarize(total_dem_rep_ratio_votes = sum(dem_rep_ratio_votes),
                                              total_pop = sum(pop_estimate)) %>%
    mutate(state_win=ifelse(as.numeric(total_dem_rep_ratio_votes)>as.numeric(total_pop),1,0))
  
  final_elect_rollup_mn <- elect_rollup_mn %>% 
  mutate(dem_electoral_votes = state_win,rep_electoral_votes = 1-dem_electoral_votes)
  
  # add up the Maine Nebraska state totals and then add them to the final rollup
  final_elect_rollup <- final_elect_rollup %>% 
    left_join(
      final_elect_rollup_mn %>% 
        group_by(state) %>% 
        summarize(
          total_dem_rep_ratio_votes = sum(total_dem_rep_ratio_votes),
          total_pop = sum(total_pop),
          dem_electoral_votes = sum(dem_electoral_votes),
          rep_electoral_votes = sum(rep_electoral_votes),
        ),
      by = c("stname" = "state"),
      suffix = c("", "_mn")
    ) %>% 
    mutate(
      total_dem_rep_ratio_votes = ifelse(!is.na(total_dem_rep_ratio_votes_mn), total_dem_rep_ratio_votes + total_dem_rep_ratio_votes_mn, total_dem_rep_ratio_votes),
      total_pop = ifelse(!is.na(total_pop_mn), total_pop + total_pop_mn, total_pop),
      dem_electoral_votes = ifelse(!is.na(dem_electoral_votes_mn), dem_electoral_votes + dem_electoral_votes_mn, dem_electoral_votes),
      rep_electoral_votes = ifelse(!is.na(rep_electoral_votes_mn), rep_electoral_votes + rep_electoral_votes_mn, rep_electoral_votes),
    ) %>% 
    select(names(.)[!str_detect(names(.), "_mn")])
  
  # Check state win for Main and Nebraska (set it to 0 since these are not winner-take-all states)
  
  # final_elect_rollup$state_win[final_elect_rollup$stname %in% c("Maine", "Nebraska")] <- 0
  
  
  return(final_elect_rollup)
    
}


electoral_college_predictions <- map(.x = list("mlr", "svm", "xgb"), .f = ~get_electoral_college(test, .x)) %>% setNames(list("mlr", "svm", "xgb"))

# Synthesize actual outcomes and add them to the list
  actual_results <- bind_rows(
    elect_tbl_actual_2020 %>% 
      mutate(
        dem_electoral_votes = state_win*elect_votes,
        rep_electoral_votes = elect_votes - dem_electoral_votes
      ),
    maine_neb_actual_2020 %>% 
      distinct(state_fips, state, congress_district, state_win) %>% 
      mutate(
        dem_electoral_votes = 1*state_win, 
        rep_electoral_votes = 1 - dem_electoral_votes
        ) %>% 
      group_by(state_fips, state, state_win) %>% 
      summarize(
        dem_electoral_votes = sum(dem_electoral_votes), 
        rep_electoral_votes = sum(rep_electoral_votes)) %>% 
      rename(fips = state_fips)
    )

  actual_results <- actual_results %>% 
    group_by(fips, state) %>% 
    summarize(
      dem_electoral_votes = sum(dem_electoral_votes), 
      rep_electoral_votes = sum(rep_electoral_votes),
      state_win = max(state_win, na.rm = T)
    ) %>% 
    rename(
      state_fips = fips,
      stname = state
    )
  
  # Fix nebraska state win
  actual_results[actual_results$stname == "Nebraska", ]$state_win <- 0
  
  electoral_college_predictions[["actual"]] <- actual_results
  
# Save out predictions for Shiny app
saveRDS(electoral_college_predictions, "../data/electoral_college_predictions.rds")



```

```{r electoral_college_results}
# Create a data frame of total ellectoral college prediction
mlr_total <- electoral_college_predictions$mlr[c("dem_electoral_votes","rep_electoral_votes")] %>% colSums()
svm_total <- electoral_college_predictions$svm[c("dem_electoral_votes","rep_electoral_votes")] %>% colSums()
xgb_total <- electoral_college_predictions$xgb[c("dem_electoral_votes","rep_electoral_votes")] %>% colSums()
actual_total <- electoral_college_predictions$actual[c("dem_electoral_votes","rep_electoral_votes")] %>% colSums()


data.frame(
  Models = c("Multiple Linear Regression", "Support Vector Machine", "XGBoost","Actual"),
  `Total Democratic Electoral Votes` = rbind(mlr_total["dem_electoral_votes"],svm_total["dem_electoral_votes"],
                                             xgb_total["dem_electoral_votes"],actual_total["dem_electoral_votes"]),
    `Total Republican Electoral Votes` = rbind(mlr_total["rep_electoral_votes"],svm_total["rep_electoral_votes"],
                                             xgb_total["rep_electoral_votes"],actual_total["rep_electoral_votes"])
) %>% kableExtra::kable() %>% kableExtra::kable_styling()
```

### Error Metrics  

We want to now look at the metrics for all the models used. For now we just visualise the predictions against the actual outcomes, and we will fill more here in due time.


```{r visualisation}
# will correct aspect ratio
ggplot(test, aes(x=dem_rep_ratio)) + geom_point(aes(y=svm)) + geom_abline(slope=1, intercept = 0, color='red') +ylim(c(0,1))

test %>% 
  select(dem_rep_ratio, mlr, svm, xgb) %>% 
  gather(pred_type, value, -dem_rep_ratio) %>% 
  ggplot(aes(x = dem_rep_ratio, y = value, color = pred_type)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0) +
  scale_color_manual(name = "Prediction\nType", values = c("darkolivegreen3", "darkorange3", "dodgerblue3")) +
  theme_bw() +
  xlab("Actual Values") +
  ylab("Predictions") +
  ggtitle("Plot of predictions against actual values\n(Ratio of Democrat to Republican Votes)")
  
```  

```{r rmse}
# Create a data frame of all root MSEs
data.frame(
  Models = c("Multiple Linear Regression", "Support Vector Machine", "XGBoost"),
  `Root MSE` = c(sqrt(mean(mlr_model$residuals^2)), 
           sqrt(mean(svm_model$residuals^2)),
           rmse(test$dem_rep_ratio,predict(xgb_model, x_test, missing = NaN)))
) %>% kableExtra::kable() %>% kableExtra::kable_styling()
```

```{r r_squared}
# Create a data frame of all R^2
data.frame(
  Models = c("Multiple Linear Regression", "Support Vector Machine", "XGBoost"),
  `R-Squared` = c(R2(test$dem_rep_ratio, test$mlr),
                  R2(test$dem_rep_ratio, test$svm),
                  R2(test$dem_rep_ratio, test$xgb)
                )
) %>% kableExtra::kable() %>% kableExtra::kable_styling()
```


