---
title: "Can we Predict the US Election?"
author: Ben Shea, Mukund Poddar, Nellie Ponarul and Saul Holding
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(e1071)
library(xgboost)
library(Metrics)
library(caret)
library(tidyverse)
```


### Model Fitting

We will load our dataset here that has been cleaned and put together with all the scripts in the 'cleaning_scripts folder'. We will then standardise the data.

Note:  we scale the test data here independently of the training data. This is non-standard practice that we deliberately choose here and the reasoning for it can be read at test_set_scaling.md.

```{r dataset, message = FALSE}
train = read_csv("../data/merged_final_2016.csv")
test = read_csv("../data/merged_final_2020.csv")
train = train[complete.cases(train),]
x_train = train[c(2:(length(train)-1))]
y_train = train[length(train)]
colnames(y_train) = 'actual'

test = test[complete.cases(test),]
x_test = test[c(2:(length(test)-1))]
y_test = test[length(test)]
colnames(y_test) = 'actual'

x_train = scale(x_train)
x_test = scale(x_test)
```

We now start using different Machine Learning models to predict the election outcomes. To keep some standardisation, we will store the actual results, and the predictions from each of the models we use into the y_train and y_test dataframes created above.


#### Multiple Linear Regression  

We selected a subset of our covariates for this model using a backwards stepwise selection:
```{r mlr, eval = F}
# Set up data as dataframe for MLR
x_train_mlr <- x_train %>% data.frame() %>% bind_cols(y_train)
x_test_mlr <- x_test %>% data.frame() %>% bind_cols(y_test)
# Fit model with backwards selected variables
covs <- names(x_train_mlr)[-length(names(x_train_mlr))]
covs <- paste0(covs, collapse = " + ")

# Generate a full model using log transform to make LINE assumptions more satisfied
mlr_full_model <- lm(eval(paste0("log(actual) ~ ", covs)), data = x_train_mlr)
# Perform backwards stepwise selection to remove insignificant predictors
step(mlr_full_model, direction = "back")
# Create final model statement (has variables from each)
mlr_model <- lm( log(actual) ~ dem_amount + rep_amount + popestimate + 
    netmig + race_white + race_black + race_hispanic + race_aac + 
    age_0_to_19_years + age_20_to_39_years + age_40_to_59_years + 
    age_60_to_79_years + financial.services.and.insurance + gasoline.and.other.energy.goods + 
    health.care + food + nonprofit + nondurable_goods + gdp_change + 
    unemployment + dem_poll_mean + dem_poll_median + dem_poll_sd + 
    rep_poll_mean + rep_poll_sd + consistency_dem + consistency_rep, 
    data = x_train_mlr)
# Save out exponentiated predictions
test$mlr = exp(predict(mlr_model, x_test_mlr))
```


#### Support Vector Machines  

This model creates a decision boundary between the classes being predicted using the datapoints on the edges of the class clusters. You can read more about SVMs on [wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine).
```{r svm, eval = F}
svm_model = svm(x=x_train, y=y_train)
test$svm = predict(svm_model, x_test)
```

#### XGBoost

This model is an optimized distributed gradient boosting library, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. Model has been hyperparameter tuned to minimize the mean squared error. For more information on the parameters check these [parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)
```{r xgboost, eval = F}
train_final <- matrix(as.numeric(data.matrix(x_train)), nrow = nrow(train))
test_final <- matrix(as.numeric(data.matrix(x_test)), nrow = nrow(test))

train_party_outcome <- train$dem_rep_ratio
train_outcome <- matrix(as.numeric(data.matrix(train_party_outcome)))

# GRID SEARCH -------------------------------------------------------------------------------
#https://www.kaggle.com/silverstone1903/xgboost-grid-search-r

searchGridSubCol <- expand.grid(subsample = c(0.6,0.75,0.9,1), 
                                colsample_bytree = c(0.5,0.6,0.8,1),
                                max_depth = c(3,4,5,6,7,8,9,10),
                                eta = c(0.001,0.01,0.1,0.2,0.3,0.4)
)

obj<- "reg:squarederror"

set.seed(123)
system.time(
  rmseErrorsHyperparameters <- apply(searchGridSubCol, 1, function(parameterList){
    
    #Extract Parameters to test
    currentSubsampleRate <- parameterList[["subsample"]]
    currentColsampleRate <- parameterList[["colsample_bytree"]]
    currentDepth <- parameterList[["max_depth"]]
    currentEta <- parameterList[["eta"]]
    currentMinChild <- 1
    xgb_model <- xgboost(data =  x_train, label=train_outcome, nrounds = 200, 
                         verbose = 0,"max.depth" = currentDepth, "eta" = currentEta,                               
                         "subsample" = currentSubsampleRate, "colsample_bytree" = currentColsampleRate
                         , print_every_n = 20, "min_child_weight" = currentMinChild,
                         early_stopping_rounds = 20,objective=obj, eval_metric="rmse"
                         )
    
    xvalidationScores <- as.data.frame(xgb_model$evaluation_log)
    trmse <- tail(xvalidationScores$train_rmse,1)
    output <- return(c(trmse, currentSubsampleRate, currentColsampleRate, currentDepth, currentEta, currentMinChild))}))

output <- as.data.frame(t(rmseErrorsHyperparameters))
varnames <- c("TrainRMSE", "SubSampRate", "ColSampRate", "Depth","eta","currentMinChild")
names(output) <- varnames
head(output)
best_params <- output[which.min(output$TrainRMSE),]
best_params #SubSampRate - 0.6, ColSampRate=1, depth=10, eta= 0.4,currentMinChild=1

# TRAINING XGBOOST MODEL-----------------------------------------------------------------------------------------------

#https://xgboost.readthedocs.io/en/latest/parameter.html
set.seed(7)
xgb_model = xgboost(data=x_train,
                    label=train_outcome,
                    missing = NaN,
                    nrounds=200,
                    verbose=0,
                    eta=best_params$eta,
                    max_depth=best_params$Depth,
                    subsample=best_params$SubSampRate,
                    min_child_weight=1,
                    colsample_bytree=best_params$ColSampRate,
                    objective=obj,
                    eval_metric="rmse",
                    )

test$xgb <- predict(xgb_model, test_final, missing = NaN)
```  

### Random Forest
The model is utilizes decision trees, bagging, and randomness by randomly selected predetermined number of predictors at each node, for predictions. Using 10 fold cross validation repeated 3 times and after determining a baseline model, we attempt to tune the model with Caret's in built grid search tuning method and with manual grid search. In the end, the model that uses Caret's grid search is the strongest model. 

```{r scaled}
# Combing scaled data with NON-SCALED RESPONSE VARIABLES
train_scaled <- x_train %>% data.frame() %>% bind_cols(y_train)
test_scaled <- x_test %>% data.frame() %>% bind_cols(y_test)
```

```{r eval=F}
# Getting number of predictors
number_predictors <- dim(train_scaled)[2] - 1

### Base model
#Setting the default tuning parameters
control <- trainControl(method='repeatedcv', number=10, repeats=3)
mtry <- sqrt(number_predictors)
tunegrid <- expand.grid(.mtry=mtry)

# Initializing and fitting the random forest
rf_model_base <- train(actual ~ ., data = train_scaled, method = 'rf',
                      metric='RMSE', tuneGrid=tunegrid, trControl = control)

### Tuning Method 1: Utilizing Caret's Grid Search Method
control <- trainControl(method='repeatedcv', number=10, repeats=3)
tunegrid_tune <- expand.grid(.mtry= c(sqrt(number_predictors), log2(number_predictors), number_predictors/3))
# Initializing and fitting the random forest
rf_model_tuned1 <- train(actual ~ ., data = train_scaled, method = 'rf',
                       metric='RMSE', tuneGrid=tunegrid_tune, trControl = control)
#save(rf_model_base,file="../saved_models/rf_model_base.rda")
#save(rf_model_tuned1,file="../saved_models/rf_model_tuned1.rda")
```

```{r rf_tune, eval=FALSE}

### Tuning Method 2: Manual Grid Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
tunegrid_tune <- expand.grid(.mtry= c(sqrt(number_predictors), log2(number_predictors), number_predictors/3))
modellist_new <- list()
seed <- 42

for (ntree in c(seq(40,600, 80))) {
    for (nodesize in c(10, 30, 60, 90)) {

    print(paste(ntree, nodesize))

    set.seed(seed)
    fit <- train(actual ~ ., data = train_scaled, method = 'rf',
                 metric='RMSE', tuneGrid=tunegrid_tune, trControl = control, ntree=ntree, nodesize=nodesize)
    key <- toString(ntree)
    modellist_new[[key]] <- fit
    }
}
# compare results


```


```{r rf_results}
# Reading results and model from manual search
modellist_new <- readRDS("../saved_models/model_list_final.rds")
results <- readRDS("../saved_models/results_rf_final.rds")
results <- resamples(modellist_new)

#load models for Random Forest base and tuned
load("../saved_models/rf_model_base.rda")
load("../saved_models/rf_model_tuned1.rda")

# Comparing the different models
rf_model_base
rf_model_tuned1
summary(results)
dotplot(results)
```

As we can see, rf_model_tuned1 performs the best with mtry=p/3. We shall use this model to predict on the test set.

```{r}
# Predicting using best model on test set and saving predictions and errors to their respecitive dataframes
test$rf <- predict(rf_model_tuned1, newdata = test_scaled)
rf_rmse <- rmse(test_scaled$actual,test$rf)
```

```{r, echo = F}
# Save out predictions so we don't run code multiple times
#save(mlr_model, svm_model, xgb_model, rf_model_tuned1, test, file = "../data/raw_predictions.rda")
# Save our rf predictions to a separate test object since it will get overwritten in the load below
test_rf <- test
load("../data/raw_predictions.rda")
# Add on rf predictions to the current test object which is used below
test <- test %>% 
  left_join(test_rf,
            by = c("fips", "dem_amount", "rep_amount", "popestimate", "netmig", "race_white", "race_black", "race_hispanic", "race_aac", "age_0_to_19_years", "age_20_to_39_years", "age_40_to_59_years", "age_60_to_79_years", "age_80_years_or_older", "financial.services.and.insurance", "gasoline.and.other.energy.goods", "health.care", "other.nondurable.goods", "personal.consumption.expenditures", "food", "household", "nonprofit", "nondurable_goods", "durable_goods", "goods_clothing_footwear", "services", "recreation", "transportation", "raw_gdp", "gdp_change", "unemployment", "dem_poll_mean", "dem_poll_median", "dem_poll_sd", "rep_poll_mean", "rep_poll_median", "rep_poll_sd", "consistency_dem", "consistency_rep", "dem_rep_ratio"))

# Save out raw predictions with random forest added in
#saveRDS(test, "../data/test_with_rf.rds")
```

### Outcomes

#### Predict Electoral College Split

```{r, warning= FALSE, message=FALSE}
# electoral college; note: Maine and Nebraska have Congressional District splits, so the roll-up is separated there 
elect_tbl <- read_csv("../data/electoral_college.csv")
maine_neb <- read_csv("../data/electoral_votes_main_nebraska.csv")

# Actual results 2020
elect_tbl_actual_2020 <- read_csv("../data/electoral_college_actual_2020.csv")
maine_neb_actual_2020 <- read_csv("../data/electoral_votes_main_nebraska_actual_2020.csv")
#read in county/state crosswalk
base_county_state_fips <- read_csv("../data/Clean Data/base_county_state_fips_lkp.csv")

# Function to take a prediction and calculate electoral college split ------------------------------------------------
get_electoral_college <- function(df, pred_input, elect_votes =  elect_tbl, maine_neb_elect = maine_neb){
  # roll up to State level
  test_pred_df <- data.frame(fips = df$fips,
                             dem_rep_ratio = df[pred_input],
                           pop_estimate = df$popestimate)

  names(test_pred_df) <- c("fips", "dem_rep_ratio", "pop_estimate")
    test_pred_df <- test_pred_df %>%
    mutate(
      dem_rep_ratio_votes = dem_rep_ratio*pop_estimate,
      state_fips = str_sub(fips, 1,2)
    ) %>%
    left_join(
      base_county_state_fips[c("stname","fips")],
      by="fips")

  #most of electoral college
  elect_rollup <- test_pred_df %>%
    left_join(
      elect_tbl,
      by = c("stname" ="state")) %>%
    group_by(state_fips, stname) %>%
    summarize(total_dem_rep_ratio_votes = sum(dem_rep_ratio_votes),
              total_pop = sum(pop_estimate), .groups="drop") %>%
    mutate(
      state_win=ifelse(total_dem_rep_ratio_votes>total_pop,1,0)
      )

  final_elect_rollup <- elect_rollup %>%
  left_join(elect_votes %>%
              select(state, elect_votes),
            by=c("stname" ="state")) %>%
  mutate(
    dem_electoral_votes = state_win*elect_votes,
    rep_electoral_votes = elect_votes - dem_electoral_votes
    )

  # Additional Maine/Nebraska Calculation
  maine_neb_elect$fips <- as.character(maine_neb_elect$fips)
  elect_rollup_mn <- maine_neb_elect %>% left_join(test_pred_df, by = c("fips","state"="stname"))
  elect_rollup_mn <- elect_rollup_mn[complete.cases(elect_rollup_mn), ] %>%
    group_by(state,congress_district) %>% summarize(total_dem_rep_ratio_votes = sum(dem_rep_ratio_votes),
                                              total_pop = sum(pop_estimate), .groups="drop") %>%
    mutate(state_win=ifelse(as.numeric(total_dem_rep_ratio_votes)>as.numeric(total_pop),1,0))

  final_elect_rollup_mn <- elect_rollup_mn %>%
  mutate(dem_electoral_votes = state_win,rep_electoral_votes = 1-dem_electoral_votes)

  # add up the Maine Nebraska state totals and then add them to the final rollup
  final_elect_rollup <- final_elect_rollup %>%
    left_join(
      final_elect_rollup_mn %>%
        group_by(state) %>%
        summarize(
          total_dem_rep_ratio_votes = sum(total_dem_rep_ratio_votes),
          total_pop = sum(total_pop), .groups="drop",
          dem_electoral_votes = sum(dem_electoral_votes),
          rep_electoral_votes = sum(rep_electoral_votes),
        ),
      by = c("stname" = "state"),
      suffix = c("", "_mn")
    ) %>%
    mutate(
      total_dem_rep_ratio_votes = ifelse(!is.na(total_dem_rep_ratio_votes_mn), total_dem_rep_ratio_votes + total_dem_rep_ratio_votes_mn, total_dem_rep_ratio_votes),
      total_pop = ifelse(!is.na(total_pop_mn), total_pop + total_pop_mn, total_pop),
      dem_electoral_votes = ifelse(!is.na(dem_electoral_votes_mn), dem_electoral_votes + dem_electoral_votes_mn, dem_electoral_votes),
      rep_electoral_votes = ifelse(!is.na(rep_electoral_votes_mn), rep_electoral_votes + rep_electoral_votes_mn, rep_electoral_votes),
    ) %>%
    select(names(.)[!str_detect(names(.), "_mn")])

  return(final_elect_rollup)

}
# calculate the electoral college for each model and combine into 1 object -----------------------------------------------

electoral_college_predictions <- purrr::map(.x = list("mlr", "svm", "xgb","rf"), .f = ~get_electoral_college(test, .x)) %>%
  setNames(list("mlr", "svm", "xgb","rf"))

# Synthesize actual outcomes and add them to the list
  actual_results <- bind_rows(
    elect_tbl_actual_2020 %>% 
      mutate(
        dem_electoral_votes = state_win*elect_votes,
        rep_electoral_votes = elect_votes - dem_electoral_votes
      ),
    maine_neb_actual_2020 %>% 
      distinct(state_fips, state, congress_district, state_win) %>% 
      mutate(
        dem_electoral_votes = 1*state_win, 
        rep_electoral_votes = 1 - dem_electoral_votes
        ) %>% 
      group_by(state_fips, state, state_win) %>% 
      summarize(
        dem_electoral_votes = sum(dem_electoral_votes), 
        rep_electoral_votes = sum(rep_electoral_votes), .groups="drop") %>% 
      rename(fips = state_fips)
    )

  actual_results <- actual_results %>% 
    group_by(fips, state) %>% 
    summarize(
      dem_electoral_votes = sum(dem_electoral_votes), 
      rep_electoral_votes = sum(rep_electoral_votes),
      state_win = max(state_win, na.rm = T), .groups="drop"
    ) %>% 
    rename(
      state_fips = fips,
      stname = state
    )
  
  # Fix nebraska state win
  actual_results[actual_results$stname == "Nebraska", ]$state_win <- 0
  
  electoral_college_predictions[["actual"]] <- actual_results
  
# Save out predictions for Shiny app
saveRDS(electoral_college_predictions, "../data/electoral_college_predictions.rds")

```

```{r electoral_college_results, message = FALSE}
electoral_college_predictions <- readRDS("../data/electoral_college_predictions.rds")

# Create a data frame of total electoral college prediction
mlr_total <- electoral_college_predictions$mlr[c("dem_electoral_votes","rep_electoral_votes")] %>% colSums()
svm_total <- electoral_college_predictions$svm[c("dem_electoral_votes","rep_electoral_votes")] %>% colSums()
xgb_total <- electoral_college_predictions$xgb[c("dem_electoral_votes","rep_electoral_votes")] %>% colSums()
rf_total <- electoral_college_predictions$rf[c("dem_electoral_votes","rep_electoral_votes")] %>% colSums()
actual_total <- electoral_college_predictions$actual[c("dem_electoral_votes","rep_electoral_votes")] %>% colSums()


electoral_results_df <- data.frame(
  Models = c("Multiple Linear Regression", "Support Vector Machine", "XGBoost","Random Forest","Actual"),
  `Total Democratic Electoral Votes` = rbind(mlr_total["dem_electoral_votes"],svm_total["dem_electoral_votes"],
                                             xgb_total["dem_electoral_votes"],rf_total["dem_electoral_votes"],
                                             actual_total["dem_electoral_votes"]),
    `Total Republican Electoral Votes` = rbind(mlr_total["rep_electoral_votes"],svm_total["rep_electoral_votes"],
                                             xgb_total["rep_electoral_votes"],rf_total["rep_electoral_votes"],
                                             actual_total["rep_electoral_votes"])
) 

electoral_results_df %>% kableExtra::kable() %>% kableExtra::kable_styling()

save(electoral_results_df, file = "../results/electoral_results_df.rda")
```

### Error Metrics  

We want to now look at the metrics for all the models used. Below are the predictions against the actual outcomes, the Root Mean Squared Error (RMSE), and the R^2 for our each model.


```{r visualisation, warning=FALSE}

test %>% 
  select(dem_rep_ratio, mlr, svm, xgb, rf) %>% 
  gather(pred_type, value, -dem_rep_ratio) %>% 
  ggplot(aes(x = dem_rep_ratio, y = value, color = pred_type)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0) +
  scale_color_manual(name = "Prediction\nType", values = c("darkolivegreen3", "darkorange3", "dodgerblue3", "bisque3")) +
  scale_x_log10() +
  scale_y_log10() +
  theme_bw() +
  xlab("Actual Values") +
  ylab("Predictions") + 
  ggtitle("Plot of predictions against actual values\n(Ratio of Democrat to Republican Votes)")
  
```  

```{r rmse}
# Create a data frame of all root MSEs
rmse_df <- data.frame(
  Models = c("Multiple Linear Regression", "Support Vector Machine", "XGBoost", "Random Forest"),
  `Root MSE` = c(sqrt(mean(mlr_model$residuals^2)), 
           sqrt(mean(svm_model$residuals^2)),
           rmse(test$dem_rep_ratio,predict(xgb_model, x_test, missing = NaN)),
           rf_rmse)
) 

rmse_df %>% kableExtra::kable() %>% kableExtra::kable_styling()
save(rmse_df, file = "../results/rmse_df.rda")
```

```{r r_squared}
# Create a data frame of all R^2
r2 <- data.frame(
  Models = c("Multiple Linear Regression", "Support Vector Machine", "XGBoost","Random Forest"),
  `R-Squared` = c(R2(test$dem_rep_ratio, test$mlr),
                  R2(test$dem_rep_ratio, test$svm),
                  R2(test$dem_rep_ratio, test$xgb),
                  R2(test$dem_rep_ratio, test$rf)
                )
) 

r2 %>% kableExtra::kable() %>% kableExtra::kable_styling()
save(r2, file = "../results/r2.rda")
```

The top 10 most important features in the xgboost model:

```{r xgboost_feat_imp, warning=FALSE}
head(xgb.importance(feature_names = NULL, 
                    model = xgb_model, data = x_train),10) %>%
  kableExtra::kable() %>% kableExtra::kable_styling()
```


